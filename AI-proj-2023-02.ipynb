import numpy as np

from sklearn import datasets
from matplotlib import pyplot as plt

%matplotlib inline

SEED = 42
np.random.seed(SEED)

num_epochs = 5
num_samples = 100
num_train_samples = 70
batch_size = 10
learning_rate = 0.01

X, y = datasets.make_blobs(n_samples=num_samples, n_features=2, centers=2, cluster_std=1.05, random_state=SEED)

mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma

fig = plt.figure(figsize=(10, 8))

plt.plot(X_train[:, 0][y_train == 0], X_train[:, 1][y_train == 0], 'r^')  # Class 0 (red triangle)
plt.plot(X_train[:, 0][y_train == 1], X_train[:, 1][y_train == 1], 'bs')  # Class 1 (blue square)
plt.xlabel("x[0]")
plt.ylabel("x[1]")
plt.title("Training data")

fig = plt.figure(figsize=(10, 8))

plt.plot(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], 'r^')  # Class 0 (red triangle)
plt.plot(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], 'bs')  # Class 1 (blue square)
plt.xlabel("x[0]")
plt.ylabel("x[1]")
plt.title("Test data")

def activation_function(x):
  return np.where(x<=0, 0, 1)

def error_function(y_pred, y):
    return np.sum(np.square(y_pred - y)) / 2

class Perceptron():
  def __init__(self, num_features=2):
    self.weights = np.random.rand(num_features, 1)
    self.bias = np.random.rand(1)
   
  def forward(self, x):
    return activation_function(np.matmul(x, self.weights) + self.bias)

neuron = Perceptron()

for e in range(num_epochs):                                         # Training loop
  error = 0.0
  for i in range(num_train_samples // batch_size):                  # Batch loop
    X_train_batch = X_train[i*batch_size:(i+1)*batch_size]
    y_train_batch = y_train[i*batch_size:(i+1)*batch_size]
    y_pred_batch = neuron.forward(X_train_batch).ravel()            # Forward
    
    error += error_function(y_pred_batch, y_train_batch)            # Error calculation
    print("Epoch " + str(e + 1) + " [" + str(i*batch_size) + \
          "/" + str(num_train_samples) + "] Error: " + str(error))
    
    new = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])                                      # d(sum(square(ypred - y)) / 2)/dw = sum((ypred-y)*xi) 를 계산하기 위한 임의의 행렬 생성
    gradient_weight = np.dot(X_train_batch.T, new)*np.sum(y_pred_batch - y_train_batch) # weight의 gradient 계산
    neuron.weights -= learning_rate * gradient_weight.reshape(2,1)                      # weight update
    gradient_bias = np.sum(y_pred_batch - y_train_batch)                                # bias의 gradient 계산 d(sum(ypred - y)) / db = sum(ypred - y) 
    neuron.bias -= learning_rate * gradient_bias                                        # bias update

#calculate prediction accuracy for training set
y_train_pred = neuron.forward(X_train).ravel()
accuracy = np.sum(y_train_pred == y_train) / y_train.size
print("Train set accuracy: %.2f%%" % (accuracy*100))

#display model parameters
print("Model parameters:")
print("  Weights: %s" % neuron.weights)
print("  Bias: %s" % neuron.bias)

#calculate prediction accuracy for test set
y_test_pred = neuron.forward(X_test).ravel()
accuracy = np.sum(y_test_pred == y_test) / y_test.size
print("Test set accuracy: %.2f%%" % (accuracy*100))

#plot decision boundary
w, b = neuron.weights, neuron.bias

x_min = -0.7
y_min = ( (-(w[0] * x_min) - b[0])
          / w[1] )

x_max = 0.01
y_max = ( (-(w[0] * x_max) - b[0]) / w[1] )


fig, ax = plt.subplots(1, 2, sharex=True, figsize=(9, 4))

ax[0].plot([x_min, x_max], [y_min, y_max])
ax[0].title.set_text("Training dataset [70%]")
ax[1].plot([x_min, x_max], [y_min, y_max])
ax[1].title.set_text("Test dataset [30%]")

ax[0].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], marker='^', c='r')
ax[0].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], marker='s', c='b')

ax[1].scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], marker='^', c='r')
ax[1].scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], marker='s', c='b')

plt.show()
    
